# -*- coding: utf-8 -*-
"""AzureTranslation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aeupAY0UFAeXbeZ0W56r8Rpa12LzbxP9
"""

!pip install requests python-docx
!pip install requests beautifulsoup4

import requests
import os
from docx import Document
from bs4 import BeautifulSoup

subscription_key = 'YOUR_SUBSCRIPTION_KEY'
endpoint = 'https://api.cognitive.microsofttranslator.com'
location = 'YOUR_API_LOCATION'
language_destination = 'pt-br'

def translator_text(text, target_language):
  path = '/translate'
  constructed_url = endpoint + path
  headers = {
      'Ocp-Apim-Subscription-Key': subscription_key,
      'Ocp-Apim-Subscription-Region': location,
      'Content-type': 'application/json',
      'X-ClientTraceId': str(os.urandom(16))
  }

  body = [{
      'text': text
  }]
  params = {
      'api-version': '3.0',
      'from': 'en',
      'to': target_language
  }
  request = requests.post(constructed_url, params=params, headers=headers, json=body)
  response = request.json()
  return response[0]["translations"][0]["text"]

translator_text("Some text", language_destination)

def translate_document(path):
  document = Document(path)
  full_text = []
  for paragraph in document.paragraphs:
    translated_text = translator_text(paragraph.text, language_destination)
    full_text.append(translated_text)

  translate_doc = Document()
  for line in full_text:
    print(line)
    translate_doc.add_paragraph(line)
  path_translated = path.replace('.docx', f'_{language_destination}.docx')
  translate_doc.save(path_translated)
  return path_translated

input_file = '/SomeFileName.docx'
translate_document(input_file)

def extract_text_from_url(url):
  response = requests.get(url)

  if response.status_code == 200:
    soup = BeautifulSoup(response.text, 'html.parser')

    for script_or_style in soup(['script', 'style']):
      script_or_style.decompose()

    text = soup.get_text(separator= '')

    # Clean text
    lines = (line.strip() for line in text.splitlines())
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    cleaned_text = '\n'.join(chunk for chunk in chunks if chunk)

    translated_text = translator_text(cleaned_text, language_destination)

    return translated_text


  else:
    print(f'Failed to fetch the URL. Status code: {response.status_code}')
    return None

  text = soup.get_text()

url = 'Some url'
translated_text = extract_text_from_url(url)

print(translated_text)